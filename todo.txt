implement gl backend 


implement metal backend


implement directx backend


implement vulkan backend 

add support for mapping sub buffers
add support for primitive arrays

write unit tests
add new examples
remove raw vulkan type accessors (vulkan_context->device etc...)


implement composite context for profiling, constructs profiler_texture2d which owns vulkan_texture and forward all calls to wrapping them with callstack timers


-- implement readback.
add function to check if job has requested writable access to buffer
add readback() function that shares same parameters as map but also requires 'decorator' to tell if readable/writable
dispatch copy command to graphics queue to copy to staging buffer
add a fence to make sure job completes 
readback has template overload that requores buffer_view to be const if not writable 

---implement preserved memory type for vulkan_buffer and vulkan image
add a wait on transfer fence to map(), it can be owned by the staging manager
dont reallocate unless there is a resize required, this goes for map/unmap as well as staged transfer
when reallocaing for map,  copy old contents into new buffer 
make sure memory has optimal flags for non write combining
switch particle example to use a preserved buffer

---add support for compute shaders
port cpu particle example to use this persistent storage to read and update particles each frame
compute batching system? need to add write barriers between compute jobs writing to the same resource, probably need to sort compute jobs topologically

---implement deferred upload. 
replace passing of barrier and queue managers with vulkan managers
remove extra forward declarations of barrier manager/queue manager
move submit frame job into vulkan managers
add staged resources to dependencies, and stage() to resource
staged resorces follow similar behavior patterns to read hazards w.r.t vulkan jobs
staged resources only active for dependencies with readable access
rename vulkan_job::submit_activated_dependency_barriers to process_activated_dependencies
add a staged transfer step to process_activated_dependencies, which flushes staged resources, adding them to read hazards as a side effect
when flushing staged resources stageing manager creates a set of all unique resources to stage in a job 
invoke on transfer on each unique resource, this in turn causes the resource to invoke push transfer passing in frame job barriers and a callback to add commands to a command buffer
push all frame job barriers to barrier manager with the command buffer
continue on to process read and write hazrds in vulkan job



//////////////////////////////////////
(CURRENT PROGRESS POINT)
//////////////////////////////////////



add wait for fence back in to back buffer end raster, but view in profiler to see where vsync waits and put fence right after. 

move vulkan abandon manager. cpp/h to /sync/

get release and relwithdebinfo working
fix shutdown crashes

fix abandon manager so it separates objects by queue and respects queue execution progress 
wrap ability to submit to vulkan_queue behind a member function so that fences can be automatically/periodically be inserted into the command stream. add ability to pass in fences, which will be inserted into the abandon queue. add a generation to fences so it is possible to tell if the fence was reset.

migrate example to use purely atmos

run exanple through new batching system

combine vulkan context member fields into config struct

add virtual validate_layout to resource, which should be called when dependencies are added, this way images can validate but buffers can ignore. use this in on_barrier when queue is null

implement abandoning for write combined buffers, remove device wait idle, wait queues etc. when uploading write combined, abandon previous gpu buffer and clear resource job state.

implement build mipmaps as vulkan job

draw target owns command buffer pool, a command buffer for each swap index, and a fence for each command buffer. this is used to prevent over running the swap count and to prevent having to dynamically create command buffers each frame (reset after waiting on the fence)

migrate pipeline code to batching system
vulkan context builds graphics piplines
build graphics pipeline cache
vulkan batch nodes selects the proper vk pipeline based on draw target size
vulkan context implements make shared for all object types, and does not memoize those that do not need it, but does allow iterating over live instances of each type of object
assert vertex parameters count is not too large in its constructor, or program if it is allocated in place within program::make 
implement draw target resizing as draw target rebinding on batches
graphics pipeline tightly bound to a draw target via batches using scoped pointer. this binding can be changed at runtime, but batches must point to a valid draw target. batches cannot be allowed to extend the lifetime of the draw target they reference
implement queue ownership transfers inside barriermanager, using n*n*2 premade semaphores
implement barrier manager submit_frame_job and frame_dependency
use vulkan context as source of truth for swap count and sample count

think about removing vulkan_pipeine_cache, maybe unnecessary. just memoize renderstates and primitives instead
finish error proofing job dependencies
vulkan_job implements begin(queue) and end() which accepts a command buffer and writes barriers to it.
add config to vulkan_job. config holds barrier manager. 

implement barriers for job dependencies

build vertex attributes descriptor to use same api as vulkan primitives descriptor
move attribute and primitive MAX_PARAMETERS next to vertex parameters and parameters respectively 
use vulkan primitives descriptor to build vulkan dependencies
- can prototype this inside vulkan synchronization by using it in void create_descriptor_sets(), and making sure storage buffer works with it 


add storage buffer to VkWriteDescriptorSet when parameter::STORAGE_BUFFER is detetected by spirv-cross

use atmos for generating descriptor sets

use vulkan primitives descriptor to build descriptor sets

add storage buffer bit to vertex, index, uniform, texture memory buffers so they can be read/ modiyed in a compute shader

rename buffer to UNIFORM_BUFFER and STORAGE_BUFFER

make sure all enums use 0 as an error sentinel
check spirv output for readonly / write only decorations





